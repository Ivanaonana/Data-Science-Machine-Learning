{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivanaonana/Data-Science-Machine-Learning/blob/main/next_ai_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "UXJWoJy_ajGA",
        "outputId": "435f117f-972c-42f6-9d4b-9f6373867eb6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In dt.py, you will implement a basic decision tree classifier for\n",
        "binary classification.  Your implementation should be based on the\n",
        "minimum classification error heuristic (even though this isn't ideal,\n",
        "it's easier to code than the information-based metrics).\n",
        "\"\"\"\n",
        "#hallo\n",
        "\n",
        "import numpy as np  # noqa: F401\n",
        "from binary import BinaryClassifier\n",
        "import util\n",
        "\n",
        "class DT(BinaryClassifier):\n",
        "    \"\"\"\n",
        "    This class defines the decision tree implementation.  It comes\n",
        "    with a partial implementation for the tree data structure that\n",
        "    will enable us to print the tree in a canonical form.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, opts):\n",
        "        \"\"\"\n",
        "        Initialize our internal state.  The options are:\n",
        "          opts.maxDepth = maximum number of features to split on\n",
        "                          (i.e., if maxDepth == 1, then we're a stump)\n",
        "        \"\"\"\n",
        "\n",
        "        self.opts = opts\n",
        "\n",
        "        # initialize the tree data structure.  all tree nodes have a\n",
        "        # \"isLeaf\" field that is true for leaves and false otherwise.\n",
        "        # leaves have an assigned class (+1 or -1).  internal nodes\n",
        "        # have a feature to split on, a left child (for when the\n",
        "        # feature value is < 0.5) and a right child (for when the\n",
        "        # feature value is >= 0.5)\n",
        "\n",
        "        self.isLeaf = True\n",
        "        self.label  = 1\n",
        "\n",
        "    def online(self):\n",
        "        \"\"\"\n",
        "        Our decision trees are batch\n",
        "        \"\"\"\n",
        "        return False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Return a string representation of the tree\n",
        "        \"\"\"\n",
        "        return self.displayTree(0)\n",
        "\n",
        "    def displayTree(self, depth):\n",
        "        # recursively display a tree\n",
        "        if self.isLeaf:\n",
        "            return (\" \" * (depth*2)) + \"Leaf \" + repr(self.label) + \"\\n\"\n",
        "        else:\n",
        "            return (\" \" * (depth*2)) + \"Branch \" + repr(self.feature) + \"\\n\" + \\\n",
        "                      self.left.displayTree(depth+1) + \\\n",
        "                      self.right.displayTree(depth+1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Traverse the tree to make predictions.  You should threshold X\n",
        "        at 0.5, so <0.5 means left branch and >=0.5 means right\n",
        "        branch.\n",
        "        \"\"\"\n",
        "\n",
        "        util.raiseNotDefined()\n",
        "\n",
        "    def trainDT(self, X, Y, used):\n",
        "        \"\"\"\n",
        "        recursively build the decision tree\n",
        "        \"\"\"\n",
        "\n",
        "        # get the size of the data set\n",
        "        N,D = X.shape\n",
        "\n",
        "        # check to see if we're either out of depth or no longer\n",
        "        # have any decisions to make\n",
        "        if self.opts['maxDepth'] <= 0 or len(util.uniq(Y)) <= 1:\n",
        "            # we'd better end at this point.  need to figure\n",
        "            # out the label to return\n",
        "            self.isLeaf = util.raiseNotDefined()    ### TODO: YOUR CODE HERE\n",
        "\n",
        "            self.label  = util.raiseNotDefined()    ### TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "        else:\n",
        "            # we need to find a feature to split on\n",
        "            bestFeature = -1     # which feature has lowest error\n",
        "            bestError   = N      # the number of errors for this feature\n",
        "            for d in range(D):\n",
        "                # have we used this feature yet\n",
        "                if d in used:\n",
        "                    continue\n",
        "\n",
        "                # suppose we split on this feature; what labels\n",
        "                # would go left and right?\n",
        "                leftY  = util.raiseNotDefined()    ### TODO: YOUR CODE HERE  # noqa: F841\n",
        "\n",
        "                rightY = util.raiseNotDefined()    ### TODO: YOUR CODE HERE  # noqa: F841\n",
        "\n",
        "\n",
        "                # we'll classify the left points as their most\n",
        "                # common class and ditto right points.  our error\n",
        "                # is the how many are not their mode.\n",
        "                error = util.raiseNotDefined()    ### TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "                # check to see if this is a better error rate\n",
        "                if error <= bestError:\n",
        "                    bestFeature = d\n",
        "                    bestError   = error\n",
        "\n",
        "            if bestFeature < 0:\n",
        "                # this shouldn't happen, but just in case...\n",
        "                self.isLeaf = True\n",
        "                self.label  = util.mode(Y)\n",
        "\n",
        "            else:\n",
        "                self.isLeaf  = util.raiseNotDefined()    ### TODO: YOUR CODE HERE\n",
        "\n",
        "                self.feature = util.raiseNotDefined()    ### TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "                self.left  = DT({'maxDepth': self.opts['maxDepth']-1})\n",
        "                self.right = DT({'maxDepth': self.opts['maxDepth']-1})\n",
        "                # recurse on our children by calling\n",
        "                #   self.left.trainDT(...)\n",
        "                # and\n",
        "                #   self.right.trainDT(...)\n",
        "                # with appropriate arguments\n",
        "                ### TODO: YOUR CODE HERE\n",
        "                util.raiseNotDefined()\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        \"\"\"\n",
        "        Build a decision tree based on the data from X and Y.  X is a\n",
        "        matrix (N x D) for N many examples on D features.  Y is an\n",
        "        N-length vector of +1/-1 entries.\n",
        "\n",
        "        Some hints/suggestions:\n",
        "          - make sure you don't build the tree deeper than self.opts['maxDepth']\n",
        "\n",
        "          - make sure you don't try to reuse features (this could lead\n",
        "            to very deep trees that keep splitting on the same feature\n",
        "            over and over again)\n",
        "\n",
        "          - it is very useful to be able to 'split' matrices and vectors:\n",
        "            if you want the ids for all the Xs for which the 5th feature is\n",
        "            on, say X[:,4]>=0.5.  If you want the corresponting classes,\n",
        "            say Y[X[:,4]>=0.5] and if you want the corresponding rows of X,\n",
        "            say X[X(:,4]>=0.5,:]\n",
        "\n",
        "          - i suggest having train() just call a second function (see below)\n",
        "            that takes additional arguments telling us how much more depth we\n",
        "            have left and what features we've used already\n",
        "\n",
        "          - take a look at the 'mode' and 'uniq' functions in util.py\n",
        "        \"\"\"\n",
        "\n",
        "        self.trainDT(X, Y, [])\n",
        "\n",
        "\n",
        "    def getRepresentation(self):\n",
        "        \"\"\"\n",
        "        Return our internal representation: for DTs, this is just our\n",
        "        tree structure -- i.e., ourselves\n",
        "        \"\"\"\n",
        "\n",
        "        return self\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN5q10Sf77/xY56yT8O0Nfx",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
